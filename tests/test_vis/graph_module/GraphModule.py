GraphModule(
  (L__self___norm1_silu): SiLU()
  (L__self___norm1_linear): Linear(in_features=4, out_features=24, bias=True)
  (L__self___norm1_norm): LayerNorm((4,), eps=1e-06, elementwise_affine=False)
  (L__self___norm1_context_silu): SiLU()
  (L__self___norm1_context_linear): Linear(in_features=4, out_features=24, bias=True)
  (L__self___norm1_context_norm): LayerNorm((4,), eps=1e-06, elementwise_affine=False)
  (L__self___attn): Attention(
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (to_q): Linear(in_features=4, out_features=4, bias=True)
    (to_k): Linear(in_features=4, out_features=4, bias=True)
    (to_v): Linear(in_features=4, out_features=4, bias=True)
    (add_k_proj): Linear(in_features=4, out_features=4, bias=True)
    (add_v_proj): Linear(in_features=4, out_features=4, bias=True)
    (add_q_proj): Linear(in_features=4, out_features=4, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=4, out_features=4, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=4, out_features=4, bias=True)
    (norm_added_q): RMSNorm()
    (norm_added_k): RMSNorm()
  )
  (L__self___norm2): LayerNorm((4,), eps=1e-06, elementwise_affine=False)
  (L__self___ff_net_0_proj): Linear(in_features=4, out_features=16, bias=True)
  (L__self___ff_net_1): Dropout(p=0.0, inplace=False)
  (L__self___ff_net_2): Linear(in_features=16, out_features=4, bias=True)
  (L__self___norm2_context): LayerNorm((4,), eps=1e-06, elementwise_affine=False)
  (L__self___ff_context_net_0_proj): Linear(in_features=4, out_features=16, bias=True)
  (L__self___ff_context_net_1): Dropout(p=0.0, inplace=False)
  (L__self___ff_context_net_2): Linear(in_features=16, out_features=4, bias=True)
)